\documentclass[a4paper,11pt]{article}

% This removes annoying dependency warning from graph packages
\RequirePackage{etex}

% Language and encoding
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{lmodern}

% Paragraph settings
\setlength{\parindent}{0cm}
\setlength{\parskip}{8pt}

% Proper date format
\usepackage[yyyymmdd]{datetime}

% For front page
\usepackage{wallpaper}

% For margin
\usepackage[margin=3cm]{geometry}

% For math
\usepackage{amsmath,amssymb,amsthm}
\usepackage{xfrac}
\usepackage{mathtools}
\usepackage{bm}

% Concrete Mathematics Font
% \usepackage[boldsans]{ccfonts}
% \usepackage[euler-digits,euler-hat-accent]{eulervm}

% Linux Libertine Font
\usepackage[T1]{fontenc}
\usepackage[tt = false]{libertine}
\usepackage{libertinust1math}

% For pseudocode
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
% \usepackage{minted}                     % Code snippets.

% For quotes
\usepackage{csquotes}

% For graphs etc.
\usepackage{tkz-berge}

% For figures
\usepackage{graphicx}
\usepackage{caption}

% For figure positioning (allows use of H)
\usepackage{float}

% For code listings
\usepackage{listings}

% For references
\usepackage[backend=biber, style=numeric, sorting=none]{biblatex}
\addbibresource{references.bib}

% For title formatting
\usepackage{titlesec, blindtext, color}

% For paragraph skipping
\usepackage{parskip}

% For appendix
\usepackage[toc,page]{appendix}

% For text coloring
\usepackage{color}

% For enumeration with letters
\usepackage{enumerate}

\input{formatting.tex}

\title{Deep Unsupervised Representation Learning on Protein Sequences using Variational Autoencoders}
\date{}

\begin{document}
\maketitle
\vspace{-1.5cm}

\section*{Master Thesis Project Descriptions}

% deep learning
% (variational) autoencoders
% protein sequences
% representation
% unsupervised
% latent space

%Deep Unsupervised Representation Learning on Protein Sequences using Variational Autoencoders

Representation learning is the task of training a machine to produce a suitable representation of features for the desired input, as opposed to manually engineering the representation. Good representations are important in order to learn useful properties of the given data.

This project revolves around applying representation learning to protein sequences. Previous attempts have applied language models to protein sequences (source), making latent representations of proteins, reaching both levels of performance comparable to the currently best known, and faster computation. A recent study \cite{rao2019evaluating} emphasizes the importance of representations in the performance on downstream tasks.

We wish to examine the performance of representations and their implications for protein informatics, by developing an unsupervised representation of protein sequences, using tools from machine learning and language modeling such as variational autoencoders and deep neural networks. The results will be compared with current state-of-the-art performance on downstream tasks.

Protein sequences are comparable to natural language sentences, in the sense that both consists of a sequence of symbols. In the case of sentences, the symbols are the letters of the alphabet, and in the case of proteins, the symbols are amino acids. Thus one might look at the results of natural language sentence representations in order to learn about protein sequence representation. A study by Google Brain \cite{bowman2015generating} showed that it is possible to represent entire sentences as single vectors in a latent space, allowing interpolation between sentences. Additionally, the latent space inherently maps common properties of sentences, such as style and topic. It is possible that a similar representation for proteins may be able to map properties of proteins, such as secondary structure.

Unsupervised learning is particularly suited for protein machine learning, since there is an overweight of unlabeled data in comparison to labeled data. If a powerful unsupervised model could be produced, the wealth of this data could be used effectively \cite{AlQuraishiUnsupervised}. Therefore, unsupervised or variants like semi- or self-supervised learning methods could be especially useful.

Initially, we wish to inspect the unsupervised UniRep model \cite{alley2019unified} which compresses protein sequences into a fixed length vector, representing the sequence in a latent space. One key aspect is that the representations created by UniRep corresponds to the internal state of the LSTM used for training. In this project, we wish to make a proper representation instead, meaning that the representation itself is the output, not the internal state. The hope is that such a model would produce an overall better representation, with better performance on standard benchmarks (?).

Current work on representations of proteins might carry potential toward a general representation of proteins, implying that general unseen protein sequences can be analyzed with such a representation with respect to functionality or structure, or at least infer commonalities from their representations. We wish to explore such implications.

% In addition, this field of research, being the understanding and formulation of protein sequence space, is comparatively new, allowing for many different ventures to be explored. This project is one step in that direction.

% I suggest that the goal of the main project should be that we try to create a proper representation, rather than simply using the internal state of the LSTM. We will do this by using variational auto encoders. The challenge here is that we need to encode *sequences* of inputs.



% Daniele
% Protein sequence models would be a great subject actually. Regarding the application, it seems pretty simple and it requires very little information in general and also for the project (Description of project in English, maximum 1,200 words. Please include background, aim and project plan and how this project is of interest to Novo Nordisk) especially if you have already proposals around.

% Wouter
% Unified rational protein engineering with sequence-only deep representation learning
% This is the first (or one of the first) applications of using language models to learn representations of protein sequences.
% https://www.biorxiv.org/content/10.1101/589333v1
% One of the authors, Mohammed Alquraishi (a collaborator of ours), also writes some of his thoughts on the method on his blog: https://moalquraishi.wordpress.com/2019/04/01/the-future-of-protein-science-will-not-be-supervised

% Evaluating Protein Transfer Learning with TAPE
% This is a very recent manuscript (will probably be published at NeurIPS this year) about representation learning of biological sequences. It is interesting because it asks the question how much different downstream tasks can be improved by first learning a good representation - and they present a benchmark set that is likely to become standard in the field (which we might use in the project).

% In the preproject, I suggest we try and reproduce the results of the first article, but with CNNs instead of RNNs. I suggest that the goal of the main project should be that we try to create a proper representation, rather than simply using the internal state of the LSTM. We will do this by using variational auto encoders. The challenge here is that we need to encode *sequences* of inputs. Here is one of the first papers that tries to do encode/decode sequences of words: https://arxiv.org/abs/1511.06349. There are a some difficulties getting this to work, and some newer references that try to resolve this, but I think you have enough reading material for now :) Again, we will try to use CNNs in addition to LSTMs, to compare pros/cons of the two approaches.

% I do not expect you to have a full overview over this literature when we meet - so don’t stress too much if there are things you don’t understand in these papers. But it would be great if you could try to write the main points of your thesis project in your own words - so we have something to discuss when we meet on the 27th.

% References
\clearpage
\printbibliography[title={References}]

\end{document}
