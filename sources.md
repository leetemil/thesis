# Source Material (Articles, etc.)

1. [Representation Learning: A Review and New Perspectives.](https://arxiv.org/pdf/1206.5538.pdf). Probably a good source for writing the the sections on representation learning.
2. [Article on tidying data.](http://vita.had.co.nz/papers/tidy-data.pdf).
3. [Google article on sentence of document representations.](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)
4. [Slides on how to evaluate ML models/methods.](http://pages.cs.wisc.edu/~dpage/cs760/evaluating.pdf)
5. [General article on Deep Learning by Turing award winners.](http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf)
6. [LEARNING PROTEIN SEQUENCE EMBEDDINGS USING INFORMATION FROM STRUCTURE.](https://openreview.net/pdf?id=SygLehCqtm)
7. [Transformers instead of RNN's](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)
8. [What is a transformer](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)
9. [Attention Mechanisms](https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129)
10. [Seq2seq Attention in Keras](https://medium.com/@jbetker/implementing-seq2seq-with-attention-in-keras-63565c8e498c)
11. [Seq2Seq Attention in PyTorch](https://towardsdatascience.com/attention-seq2seq-with-pytorch-learning-to-invert-a-sequence-34faf4133e53)
12. [VAE in Pytorch](https://vxlabs.com/2017/12/08/variational-autoencoder-in-pytorch-commented-and-annotated/)
13. [Sequence learning in Keras](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)
14. [Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
15. [Drawing NN figures](http://alexlenail.me/NN-SVG/LeNet.html)
16. [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
