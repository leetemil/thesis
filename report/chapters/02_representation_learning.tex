\chapter{Representation Learning}
In machine learning, algorithms and learning methods are applied to datasets. The dataset is stored as binaries on disk, but are usually fed to the machine learning models in some other structured format, like data frames or arrays. Data processing and representation is a core area in learning and performance of machine learning models. Depending on the representation, patterns and explaining regularities in the dataset might be more or less obscured to the model \cite{bengio2013representation}. For this reason, finding a good representation of data is nontrivial and often a learning task itself. Usually, this involves various preprocessing and cleaning of the raw data samples, as well as engineering features that capture well the crucial information. In protein engineering, such cleaning steps may include multiple sequence alignments of protein families and removal of certain noisy/insignificant parts of the protein sequence, while feature engineering could be the step from low-level representation of protein sequences to more abstract features containing fundamental protein properties such as stability and secondary structure. Manually engineering features does not scale well and throttles the application of machine learning models. For this reason, learning suitable representations is a task of major importance in machine learning, often applied internally in a model to transform the input data before passing it to a predicting component such as a classifier. In other cases a machine learning model is built with the sole task of figuring out good representations that can then subsequently be applied to downstream learning tasks as input. Of course, \textit{good} representations depend on the task at hand, but often they will capture abstract, general features of the data. Such representations can often be captured without a task in mind, i.e. by learning from \textit{unsupervised} data. Such representations are discussed in section \ref{sec:supervised_vs_unsupervised}. 

In probabilistic settings, representations are distributed in a feature space, or \textit{latent space}, such that a given data sample has an associated uncertainty. These representations are discussed in section \ref{seq:latent_space}.

\section{Supervised vs. Unsupervised Learning}
\label{sec:supervised_vs_unsupervised}

A \textit{supervised} learning setting consists of a data set $\crts{\prts{\ve{x}_i, \ve{y}_i}}^N_{i=1}$ of $N$ pairs consisting of a data sample $\ve{x}_i$ and its target $\ve{y}_i$ (also called a label of the data sample). The learning task is to infer the function that maps from samples to targets, based on the available data set. This includes unseen samples, and so a successfully learned function generalizes to all samples, not just the observed. In these settings, learned representations has a clear objective; they should aid the supervised learning task. However, supervised learning usually requires some processing of data to obtain its associated targets (a supervision), requiring human interaction. This dependency has the undesirable effect that labeled data sets are in many cases expensive and small. Therefore, in many practical settings, supervised learning is a constraint on the size of the available data set, effectively excluding any available data which does not have associated targets. Such constraints can lead to extreme data inefficiency, as in the case of protein sequences shown in figure \ref{fig:data_increase}. Here the Protein Data Bank (PDB) contains the labeled data, in this case the protein sequence and its 3D structure, and UniRef50 and UniParc contain raw sequences only. The amount of available unlabeled data is staggering compared to the amount of labeled available data. This difference alone is a strong incentive to explore what we can do in unsupervised learning settings. 

\begin{figure}[ht]
    \centering
    \includegraphics{report/figures/alt-fig.png}
    \caption{labeled vs unlabeled data increase \todo{ref this figure to https://moalquraishi.wordpress.com/2019/04/01/the-future-of-protein-science-will-not-be-supervised/ or make our own}}
    \label{fig:data_increase}
\end{figure}

In an \textit{unsupervised} learning setting, data samples have no associated targets. The data set is $\crts{\ve{x}_i}_{i=1}^N$ and the learning task is to find any underlying structure or pattern in the data. Basically, we look for any commonalities or general information in the data set. This is typically information such as the distribution of the data set or detection of clusters within the data. This is not trivial, as there is no obvious learning response, i.e. how does the machine know if it has found anything meaningful when there is no labels to guide the process. In the autoencoding approach, adopted in this thesis and discussed in section \ref{seq:autoencoders}, the learning task simulates a supervised setting by reconstructing an input data sample from a compressed representation. That is, the data sample is its own target. In the context of representation learning, the underlying assumption is that in order for the model to learn to reconstruct its input from a compressed representation, the representation must necessarily retain properties of the sample that identify it the most. Making the representation compressed is essential, as the model could otherwise naively pass the sample unaltered through its internal components, effectively learning the identity function $f(\ve{x}) = \ve{x}$. The model learns to reconstruct the input samples, having the side-effect that it produces internal representations of the data samples. 


% \todo{discuss this excerpt}
% From \href{https://moalquraishi.wordpress.com/2019/04/01/the-future-of-protein-science-will-not-be-supervised/}{blog}
% \textit{"On the one hand this may seem depressing, but on the other hand, I believe it presents a unique opportunity for unsupervised and semi-supervised machine learning. I am aware of no other problem in which the gap between labelled and unlabeled data is this large and continually increasing. If unsupervised learning can be made to work somewhere, it ought to be here. I emphasize this point because I have followed unsupervised learning with some interest over the last few years, and found most applications to be somewhat uncompelling, in the sense that the increase in performance gained from e.g. unsupervised initialization of an RNN always seems to be marginal. In many applications this is further exacerbated by the fact that acquiring labelled data is not all that expensive, rendering the extra effort that goes into semi-supervised learning even less worthwhile. However, the gap between labelled and unlabelled data in most applications is on the scale of 1 to 2 orders of magnitude, at most. What we see in protein sequence vs. e.g. structure is a gap of 5 orders of magnitude. This suggests, again, that if unsupervised learning were to work anywhere, it ought to be on proteins.}

% \textit{Another advantage of proteins is the wealth of prior knowledge that can be exploited to construct sophisticated loss functions for unsupervised learning, instead of simple next letter prediction (approaches like BERT have gone beyond this, but the amount of unsupervised signals in NLP seem to be much more limited than proteins.)"}

\section{Representations}
A fundamental assumption in machine learning settings is that a hierarchical modeling is possible: explaining aspects of the learning problem can be broken into less and less abstract sub-factors that are ultimately founded in the raw sample input. That is, when we use a machine learning model on a learning task, it represents a hierarchical representation of how the explaining factors of the data is related, starting from concrete, low-level features of the input and becoming gradually more abstract as we pass through the model toward its output. Explaining factors are what a good representation contains, implying that such representations can be learned using neural networks that captures the representation hierarchy.

Another assumption we have on the representation space is that it is smooth: if data samples $x_1$, and $x_2$ are close (by some distance metric), then so are their representations $r(x_1)$ and $r(x_2)$. This is central to the idea that similar samples end up having similar representations. In exploratory settings, this implies that we should look in the neighborhood of a sample if we wish to observe close modifications of that sample.

In addition, any data set we are likely to use is a product of many, complex processes that interact in a shared context. Explanatory factors of the data set are therefore \textit{entangled}, meaning that they are highly dependent and thus hard to separate (look at independently). Because the data is generated from such complex factors, learning good representations involve \textit{disentangling} them to features that still captures the variation of the data, but are not dependent on each other. This does not however mean that the learned representation features are easily understood by humans. Usually, representations learned by the deep learning model are complex and hard to interpret by humans. Often, the model is like a black box that somehow figures out how to transform the data into appropriate abstract representations, and those representations are then evaluated based on their metrics/performance, rather than interpretation of what the representations are. Ideally, a good representation also provides this aspect of interpretability for humans.

% \todo{3.4: reason for using deep learning to make representations: we assume the representations are hierarchical, i.e. that the explanatory factor can be constructed from other explanatory factors that are less abstract. deep representations start from very simple representations (input) and build more and more abstract representations in a hierarchy through the deep model, ultimately ending up with the final abstract representation on top, as the output.}

% \todo{smoothness of feature space. we assume that if data samples $x_1$, and $x_2$ are close, then so are their representations $r(x_1)$ and $r(x_2)$.}

% \todo{3.5: disentangling explanatory factors: data is generated from many underlying factors. these are entangled, i.e. hard do separate and dependent. we wish to disentagle them to features that capture the variance of the data}

% \todo{usually, representations learned by the deep learning model are complex and hard to interpret by humans. the model is like a black box that magically figures out how to transform the data into appropriate abstract representations, and those representations are evaluated based on their metrics/performance, rather than interpretation of what the representation is. ideally, a good representation provides independent dimensions of the data, which can be understood by humans. 'what has the model actually learned?'}


% \todo{dimensionality reduction and compression of data}

\subsection{Latent Spaces}
\label{seq:latent_space}

\subsection{Global vs. Local Representations}
\todo{discuss this excerpt}
From \href{https://moalquraishi.wordpress.com/2019/04/01/the-future-of-protein-science-will-not-be-supervised/}{blog}:
%\textit{"A key feature of this type of representation learning is its induction of a global representation of protein sequence space. Much of the prior work in this space has focused on family-specific representations, for example VAE-based ones. While family-specific representations have proven to be very powerful, particularly for protein structure prediction and for predicting the effects of mutant variants, they have always left me feeling somewhat unsatisfied. I say this because from the perspective of data / sampling efficiency, they’re only able to exploit patterns observed within a single protein family. They fragment protein sequence space into local clusters and perform learning, unsupervised or otherwise, separately for each family. This sort of sample vs. model complexity tradeoff is emblematic of much of machine learning, and I’ve written about it before in the context of predicting SH2-mediated protein-protein interactions. On the one hand, if each protein family is learned separately, the complexity of the model is reduced. But the amount of data available is also reduced, fracturing the inherent universality of proteins into tiny phenomenological universes. On the other hand, a global model of protein sequence space is able to leverage all available data, but must learn something truly general, a much more demanding task that substantially increases model complexity. What is the right tradeoff—where is the sweet spot? My hunch has long been that a global model is likely to be more performant. And with UniRep, I believe we’re beginning to see this play out."}

%\textit{"Beyond the question of performance, a global model of protein sequence space has the potential to be much more useful, by being more broadly applicable. The challenge we face in protein informatics, and protein science more broadly, is that our functional characterization of proteins is sparse and patchy. Certain protein families have had the benefit of deep functional characterization, both in terms of gross function (e.g. the plethora of mammalian signaling proteins) and in terms of detailed structural perturbation (e.g. as resultant from deep mutational scans). The GFP family of proteins studied in the UniRep paper is an example of one such family. This patchiness makes it difficult to say something truly useful about the vast majority of proteins (across Prokarya and Eukarya) because, to use an overextended metaphor, they exist in a dark region of sequence space. And the detailed characterization we have of a few families ends up being largely uninformative about this larger space, except in a vague conceptual way."}

%\textit{"Global models of protein sequence space have the potential to change this because, if we can get them to work well, they can at a minimum help us see the connective tissue that underpins protein space, and thereby relate information about well-characterized protein families to poorly characterized ones. In essence, they provide a fancy version of k-nearest neighbors, by densely populating the empty space surrounding sparsely characterized proteins, enabling functional associations to be transferred from one protein to another much farther than previously possible."}

%\textit{"Beyond this, global models of protein sequence hold the possibility of learning something truly general about proteins, that would move us beyond mere k-nearest neighbor matching to something more akin to a linguistics of proteins, decomposing protein sequences into their constituent functional and structural fragments. The fact that UniRep learned something about protein secondary structure is an indication that this is possible and already happening, without any supervision. This is important because unlike secondary structure, most of the principles of protein function (and perhaps structure) remain opaque to us, and so our ability to perform supervision will remain limited for the foreseeable future."}

\todo{see if DL chapter 15 on representation learning has something useful}