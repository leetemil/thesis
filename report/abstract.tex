\begin{abstract}
Proteins are complex macro-molecules used extensively in living organisms and in the biological industries. Their ubiquity necessitates their exploitation -- the synthesis of useful proteins can lead to advances in medicine and other organic chemical products. However, this synthesis requires a thorough understanding, leading to the field of protein engineering.

Representation learning is an interesting avenue for protein engineering, as good representations facilitate protein analysis and may aid an exploration of the space of proteins. We perform experiments that evaluate and compare three recent protein deep learning architectures: (1) UniRep, a recurrent model, (2) the variational autoencoder, a latent space model, and (3) WaveNet, a convolutional model.

We show that the variational autoencoder achieves superior performance on mutation effect prediction for local protein families and argue that its representations display desirable properties for exploration, not exerted by the other models. Representations produced by UniRep outperform WaveNet on general protein analysis tasks, while WaveNet shows promise on mutation effect prediction.

% In this thesis, we tackle the problem of protein engineering from the perspective of machine learning. Machine learning requires large amounts of data to be effective, which luckily is readily available from online protein databases. However, most of this data is not annotated with experimental data, constraining the effective techniques to unsupervised learning, specifically in this case, representation learning. Representation learning is an interesting avenue for protein engineering, as it may facilitate an exploration of the space of proteins. Representation learning benefits greatly from a probabilistic modelling approach, whose theory we present. To explore the merits of deep representation learning on the protein engineering problem, we explore three distinct models and evaluate them on established downstream tasks. 
\end{abstract}

% \section{\textcolor{red}{final boss todo}}
% \begin{enumerate}
    % \item write conclusion
    % \item write abstract
    % \item write acknowledgements
    % \item make deepsequence figures
    % \item Thoroughly check that every single research question is explicitly answered. probably in the conclusion and abstract.
    % \item Read the thesis contract and sanity check that we actually do what we wrote we would. Make sure that censor/wouter can easily tick all the boxes when they read it by making the relevant sections stand out (bold font or explicit titles maybe).
    % \item fix in-text citations to be textcite instead of cite
    % \item get rid of todos in introduction
    % \item get rid of todos in representation learning
    % \item get rid of todos in probabilistic modelling
    % \item get rid of todos in sequence learning
    % \item get rid of todos in models
    % \item get rid of todos in experiments
    % \item get rid of todos in discussion
    % \item grammar nazi thesis
    % \item finetune all sections, i.e. read and correct
% \end{enumerate}